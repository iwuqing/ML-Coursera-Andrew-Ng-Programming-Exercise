{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Loading and Visualizing Data"
   ]
  }, 
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "data = scipy.io.loadmat(\"ex3data1.mat\")\n",
    "X = data[\"X\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 Randomly select 1 data points to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# m:  the number  of train data \n",
    "# n:  the number of  input feature\n",
    "m, n = X.shape\n",
    "\n",
    "rand_indices = random.sample(range(0, m+1), m)\n",
    "sel = X[rand_indices[0 : 1], :].reshape(20, 20)\n",
    "tempsel = np.zeros([20, 20])\n",
    "k = 0\n",
    "for i in range(0, 20):\n",
    "    for j in range(0, 20):\n",
    "        tempsel[j, i] = sel[i, j]\n",
    "plt.imshow(tempsel, cmap=plt.cm.copper)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Vectorize Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 The implementation of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 +np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 The implementations of Cost and Gradient Funciton for Regularized Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(theta, X, y, ilambda=0):\n",
    "    m, n = X.shape # the number of training example and the number of features.\n",
    "    temp_theta = theta[1:]\n",
    "    J = (-1/m) * (np.dot(y.T, np.log(sigmoid(np.dot(X, theta)))) + np.dot((1-y).T, np.log(1-sigmoid(np.dot(X, theta))))) + (ilambda/(2*m))*np.dot(temp_theta.T, temp_theta)\n",
    "    return J\n",
    "   \n",
    "def compute_grad_reg(theta, X, y, ilambda):\n",
    "    m, n = X.shape # the number of tra(ining e/(2*m))xample and the number of features\n",
    "    temp_theta = theta[1:]\n",
    "    grad = np.zeros([n, 1])\n",
    "    grad[0] = (1/m) * (np.dot(X[:, 0:1].T, (sigmoid(np.dot(X, theta))-y)))\n",
    "    grad[1:] = (1/m) * (np.dot(X[:, 1:].T, (sigmoid(np.dot(X, theta))-y))) + (ilambda/m)*temp_theta\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Test lrCostFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lrCostFunction with regularization\n",
      "Cost:  [[2.5348194]]\n",
      "Expected cost: 2.534819\n",
      "Gradients:\n",
      "[[ 0.14656137]\n",
      " [-0.54855841]\n",
      " [ 0.72472227]\n",
      " [ 1.39800296]]\n",
      "Expected gradients:\n",
      " 0.146561\n",
      " -0.548558\n",
      " 0.724722\n",
      " 1.398003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing lrCostFunction with regularization\")\n",
    "\n",
    "theta_t = np.array([[-2, -1, 1, 2]]).T\n",
    "X_t = np.hstack((np.ones([5, 1]), np.linspace(1, 15, num=15).reshape(3, 5).T/10))\n",
    "y_t = np.array([[1, 0, 1, 0, 1]]).T\n",
    "lambda_t = 3\n",
    "J = compute_cost_reg(theta_t, X_t, y_t, lambda_t)\n",
    "grad = compute_grad_reg(theta_t, X_t, y_t, lambda_t)\n",
    "print('Cost: ', J);\n",
    "print('Expected cost: 2.534819');\n",
    "print('Gradients:');\n",
    "print( grad);\n",
    "print('Expected gradients:');\n",
    "print(' 0.146561\\n -0.548558\\n 0.724722\\n 1.398003\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: One-vs-All Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 The implementation of ioptimize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "def ioptimize(X, y, initial_theta, ilambda, iters=100):\n",
    "    result = optimize.minimize(compute_cost_reg, initial_theta, args=(X, y, ilambda), method='BFGS', options={\"maxiter\": iters, \"disp\": True})\n",
    "    theta = np.array([result.x])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 The implementation of oneVsAll function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def oneVsAll(X, y, num_labels, ilambda):\n",
    "    m, n = X.shape\n",
    "    all_theta = np.zeros([num_labels, n+1])\n",
    "    X = np.hstack((np.ones([m, 1]), X))\n",
    "    for i in range(0, num_labels):\n",
    "        initial_theta = np.zeros([n+1, 1])\n",
    "        pos_index,_  = np.where(y==i+1)\n",
    "        temp_y = np.zeros([m, 1])\n",
    "        temp_y[pos_index, 0:] = 1\n",
    "        all_theta[i, :] = ioptimize(X, temp_y, initial_theta, ilambda, 50)\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.023312\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.068360\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.071812\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.050061\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.075951\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.032799\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.045323\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.093309\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.087920\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.016445\n",
      "         Iterations: 50\n",
      "         Function evaluations: 20553\n",
      "         Gradient evaluations: 51\n"
     ]
    }
   ],
   "source": [
    "ilambda = 0.1\n",
    "input_layer_size  = 400\n",
    "num_labels = 10\n",
    "all_theta = oneVsAll(X, y, num_labels, ilambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Predict for One-Vs-All "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1 The implementation of predictOneVsAll funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(theta, X):\n",
    "    # the number of labels\n",
    "    number_labels, _  = theta.shape\n",
    "    # the number of example and  feature: m, n\n",
    "    m, n = X.shape\n",
    "    p = sigmoid(np.dot(X, theta))\n",
    "    _, p = np.where(p==np.array([np.max(p, axis=1)]).T)\n",
    "    return p +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2 Test for predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.97      0.97       500\n",
      "           2       0.93      0.90      0.91       500\n",
      "           3       0.93      0.90      0.92       500\n",
      "           4       0.94      0.94      0.94       500\n",
      "           5       0.90      0.90      0.90       500\n",
      "           6       0.95      0.97      0.96       500\n",
      "           7       0.94      0.93      0.94       500\n",
      "           8       0.90      0.91      0.91       500\n",
      "           9       0.90      0.91      0.90       500\n",
      "          10       0.97      0.99      0.98       500\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      5000\n",
      "   macro avg       0.93      0.93      0.93      5000\n",
      "weighted avg       0.93      0.93      0.93      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = predictOneVsAll(all_theta.T, np.hstack((np.ones([m, 1]), X)));\n",
    "print(classification_report(y, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
